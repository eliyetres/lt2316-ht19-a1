import argparse
import os
import pickle
import sys
import time

import numpy as np
import torch
from sklearn.metrics import classification_report

from dataloader import Dataset
from get_data import create_encoding, create_prefixes, gen_data, load_data
from utils import load_pickle

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

print("Loading data...")
X,y = load_data("x_test.txt", "y_test.txt")
#X, y = load_data(args.x_file, args.y_file)
vocab = load_pickle("vocab") # open the saved vocab file created in training
print("Finishing loading data.")

print("Encoding test data...")
X_encoded = create_encoding(X, vocab)
print("Finished encoding test data.")

#print("Loading model from {}.".format(args.model_name))
print("Loading model...")
#pre_trained_model = load_model(args.model_name)
pre_trained_model = load_pickle("model") # load the model from disk
print("Finished loading model.")

# Generate data
print("Generating data...")
X_gen,y_gen = gen_data(X_encoded,y)
print("Number of sentences: ", len(X_gen))
print("Finishing processing test data.")


# Output variables
actual_language = []
predicted_language = []

# Write a testing script that takes the model generated by the training script and testing instances and class labels as arguments, 
# analogous to the training script. 
for sentence, label in zip(X_gen,y_gen):
        sentence_prefixes = len(sentence) # number of tries for prediction
        for i,prefix in enumerate(sentence):
            sent_no = i+1 # current part of sentence to predict
            print(i+1, prefix) # prefixes 

            X_train = torch.stack([prefix])
            X_train = torch.LongTensor(X_train).to(device)
            hidden_layer = pre_trained_model.init_hidden(len(prefix))

            output, _ = pre_trained_model(X_train, hidden_layer)
            _, prediction = torch.max(output.data, dim=1)
            # for each testing instance, print whether at any character prefix length, the most probable class was the correct one.
            if prediction == label:
                print("Correct!", prediction, label)
            else:
                print("Incorrect!", prediction, label)
    

# for sentence in X_test:
#     print(len(sentence))
#     sentence_prefixes = create_prefixes(sentence)
#     for i,prefix in enumerate(sentence_prefixes):
#         sent_no = i+1
#         print(i+1, prefix) # prefixes 
#         # create indices
#         indices = encodings(vocab,prefix)
#         indices = torch.LongTensor(np.array([indices])).to(device)
#     print("Length of sentence when correctly predicted: {}".format(sent_no))



# def predict(model,vocab,device, x_test, y_test):
#     indices = [bl.vocabindex[word] for word in prefix.split()]
#     indices = torch.LongTensor(np.array([indices])).to(dev)
#     # We need to add a trivial sentence length parameter to the model.
#     result = model((indices, torch.LongTensor([1]).to(dev)))
#     return result



 
# Document the command line arguments, including any you needed to add.  
# The testing script should do the following:


# the overall accuracy in terms of how often the correct class was ever the most probable at any character prefix length.
# the number of characters until hit score for all those instances where the highest probability was ever the correct class.
# the mean average number of characters until hit score.
# the number of instances for which the correct class was never the highest.





# # Validation
#     with torch.set_grad_enabled(False):
#         for local_batch, local_labels in validation_generator:
#             # Transfer to GPU
#             local_batch, local_labels = local_batch.to(device), local_labels.to(device)

#             # Model computations
#             [...]


# predictions = predict(trained_batch, sentence)
# predictions[0][-1].topk(1)[1].item()

# parser = argparse.ArgumentParser(description="Tests the model.")

# parser.add_argument("-m", "--model_name", metavar="m", dest="model_name", type=str, help="The previously saved network model.")

# args = parser.parse_args()


